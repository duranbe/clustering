{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification using  Naive Bayes\n",
    "### Based on IMDB dataset\n",
    "\n",
    "Source Annexe : https://web.stanford.edu/~jurafsky/slp3/slides/7_NB.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import glob\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list=glob.glob(pathname=\"./data/movie-reviews-en/train/pos/*.txt\")\n",
    "neg_list=glob.glob(pathname=\"./data/movie-reviews-en/train/neg/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_word = 10000    #Nb of words to keep in the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get all text files as a list of string\n",
    "def get_text_list(file_list):\n",
    "    text_list = []\n",
    "    \n",
    "    for file in file_list:\n",
    "        \n",
    "        with open(file,'r') as f:\n",
    "            text_list.append(f.read())\n",
    "            \n",
    "    return(text_list)\n",
    "        \n",
    "pos_text = ' '.join(get_text_list(pos_list))\n",
    "neg_text = ' '.join(get_text_list(neg_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Preprocessing\n",
    "\n",
    "Remove punctuation and line escape char '\\n'\n",
    "\n",
    "Then we only keep the n_word most occuring word across all text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_text = pos_text.translate(str.maketrans('','', string.punctuation))\n",
    "pos_text = pos_text.replace('\\n','')\n",
    "pos_count = dict(Counter(pos_text.split()).most_common(n_word))\n",
    "\n",
    "neg_text = neg_text.translate(str.maketrans('','', string.punctuation))\n",
    "neg_text = neg_text.replace('\\n','')\n",
    "neg_count = dict(Counter(neg_text.split()).most_common(n_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(pos_count.keys())[90:100]\n",
    "values = list(neg_count.values())[90:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21b095d0b88>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAWuElEQVR4nO3de5RVZ33G8e8TyF2TgBlYGIhgOkZBG2ymaMQ7muCVWEMl9YIuKtrirUuNiVabqmOjWVW7tLTFGB1voZNoCuYPI2tiQsQYMkSScAkFQ4QpCGO8JWpRyK9/7HfKznDOzJ45Z4bh9fmsNWvv/Z537/c3e5/znD37XEYRgZmZ5eW4o12AmZk1n8PdzCxDDnczsww53M3MMuRwNzPL0PijXQDAmWeeGdOnTz/aZZiZHVM2bNjws4hoqXXbmAj36dOn093dfbTLMDM7pkj6Sb3bfFnGzCxDDnczsww53M3MMuRwNzPL0KDhLulcSRtLP7+W9B5JEyWtkbQ9TSeU1rlC0g5J2yRdNLK/gpmZ9TdouEfEtoiYHRGzgfOB3wI3ApcDXRHRCnSlZSTNBBYBs4D5wHJJ40aofjMzq2Gol2XmAT+OiJ8AC4CO1N4BXJzmFwArI+JAROwEdgBzmlGsmZlVM9RwXwRcl+YnR8RegDSdlNrPAnaX1ulJbY8haamkbkndvb29QyzDzMwGUjncJZ0AvBq4frCuNdqO+NL4iFgREW0R0dbSUvMDVmZmNkxD+YTqy4C7I2JfWt4naUpE7JU0Bdif2nuAaaX1pgJ7hlLU+e//ylC6D9uGq99U97ZdH33GqNQAcPZH7hu1sczsj8NQwv1SDl+SAVgNLAauStNVpfZvSPo08ESgFVjfeKl/nOZ+bu6ojLPunetGZRwzGx2Vwl3SKcBLgbeVmq8COiUtAXYBCwEiYrOkTmALcBBYFhGHmlq1jarbnv+CURvrBWtvq3vb59/77VGp4R3//KpRGcdsJFUK94j4LfCEfm0PUbx7plb/dqC94erMzGxY/AlVM7MMOdzNzDI0Jr7P3exY0f6GS0ZtrA997YZRG8vy4zN3M7MMOdzNzDLkyzJmx6Ct7beMyjhP+9CLR2Ucaz6fuZuZZchn7mY2LFdeeWWWY+XCZ+5mZhnymbuZHdM6rx+dfxfxlwuPra/IcribmTXovBtuHrWx7rmk2n8u9WUZM7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDJUKdwlnSHpBkn3S9oq6QJJEyWtkbQ9TSeU+l8haYekbZKqfVbWzMyapuqZ+78A34mIpwLnAVuBy4GuiGgFutIykmYCi4BZwHxguaRxzS7czMzqGzTcJZ0GPB/4IkBE/D4ifgksADpStw7g4jS/AFgZEQciYiewAxidr20zMzOg2pn7k4Fe4EuSfiTpGkmnApMjYi9Amk5K/c8CdpfW70ltjyFpqaRuSd29vb0N/RJmZvZYVcJ9PPBnwL9FxDOB35AuwdShGm1xREPEiohoi4i2lpaWSsWamVk1VcK9B+iJiDvT8g0UYb9P0hSANN1f6j+ttP5UYE9zyjUzsyoGDfeI+CmwW9K5qWkesAVYDSxObYuBVWl+NbBI0omSZgCtwLH1L0zMzI5xVf8T0zuBr0s6AXgAeAvFE0OnpCXALmAhQERsltRJ8QRwEFgWEYeaXrmZmdVVKdwjYiPQVuOmeXX6twPtDdRlZmYN8CdUzcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDDnczcwy5HA3M8uQw93MLEMOdzOzDFUKd0kPSrpP0kZJ3altoqQ1kran6YRS/ysk7ZC0TdJFI1W8mZnVNpQz9xdFxOyIaEvLlwNdEdEKdKVlJM0EFgGzgPnAcknjmlizmZkNopHLMguAjjTfAVxcal8ZEQciYiewA5jTwDhmZjZEVcM9gO9K2iBpaWqbHBF7AdJ0Umo/C9hdWrcntZmZ2SgZX7Hf3IjYI2kSsEbS/QP0VY22OKJT8SSxFODss8+uWIaZmVVR6cw9Ivak6X7gRorLLPskTQFI0/2pew8wrbT6VGBPjW2uiIi2iGhraWkZ/m9gZmZHGDTcJZ0q6fF988CFwCZgNbA4dVsMrErzq4FFkk6UNANoBdY3u3AzM6uvymWZycCNkvr6fyMiviPpLqBT0hJgF7AQICI2S+oEtgAHgWURcWhEqjczs5oGDfeIeAA4r0b7Q8C8Ouu0A+0NV2dmZsPiT6iamWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWXI4W5mliGHu5lZhhzuZmYZcribmWWocrhLGifpR5JuSssTJa2RtD1NJ5T6XiFph6Rtki4aicLNzKy+oZy5vxvYWlq+HOiKiFagKy0jaSawCJgFzAeWSxrXnHLNzKyKSuEuaSrwCuCaUvMCoCPNdwAXl9pXRsSBiNgJ7ADmNKdcMzOrouqZ+2eBy4BHS22TI2IvQJpOSu1nAbtL/XpS22NIWiqpW1J3b2/vkAs3M7P6Bg13Sa8E9kfEhorbVI22OKIhYkVEtEVEW0tLS8VNm5lZFeMr9JkLvFrSy4GTgNMkfQ3YJ2lKROyVNAXYn/r3ANNK608F9jSzaDMzG9igZ+4RcUVETI2I6RQvlN4SEW8AVgOLU7fFwKo0vxpYJOlESTOAVmB90ys3M7O6qpy513MV0ClpCbALWAgQEZsldQJbgIPAsog41HClZmZW2ZDCPSJuBW5N8w8B8+r0awfaG6zNzMyGyZ9QNTPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPL0KDhLukkSesl3SNps6R/TO0TJa2RtD1NJ5TWuULSDknbJF00kr+AmZkdqcqZ+wHgxRFxHjAbmC/p2cDlQFdEtAJdaRlJM4FFwCxgPrBc0riRKN7MzGobNNyj8EhaPD79BLAA6EjtHcDFaX4BsDIiDkTETmAHMKepVZuZ2YAqXXOXNE7SRmA/sCYi7gQmR8RegDSdlLqfBewurd6T2vpvc6mkbkndvb29jfwOZmbWT6Vwj4hDETEbmArMkfT0Abqr1iZqbHNFRLRFRFtLS0u1as3MrJIhvVsmIn4J3EpxLX2fpCkAabo/desBppVWmwrsabhSMzOrrMq7ZVoknZHmTwZeAtwPrAYWp26LgVVpfjWwSNKJkmYArcD6ZhduZmb1ja/QZwrQkd7xchzQGRE3SboD6JS0BNgFLASIiM2SOoEtwEFgWUQcGpnyzcyslkHDPSLuBZ5Zo/0hYF6dddqB9oarMzOzYfEnVM3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMuRwNzPLkMPdzCxDDnczsww53M3MMjRouEuaJul7krZK2izp3al9oqQ1kran6YTSOldI2iFpm6SLRvIXMDOzI1U5cz8IvDcingY8G1gmaSZwOdAVEa1AV1om3bYImAXMB5ZLGjcSxZuZWW2DhntE7I2Iu9P8w8BW4CxgAdCRunUAF6f5BcDKiDgQETuBHcCcZhduZmb1Demau6TpwDOBO4HJEbEXiicAYFLqdhawu7RaT2rrv62lkroldff29g69cjMzq6tyuEt6HPBN4D0R8euButZoiyMaIlZERFtEtLW0tFQtw8zMKqgU7pKOpwj2r0fEt1LzPklT0u1TgP2pvQeYVlp9KrCnOeWamVkVVd4tI+CLwNaI+HTpptXA4jS/GFhVal8k6URJM4BWYH3zSjYzs8GMr9BnLvBG4D5JG1PbB4GrgE5JS4BdwEKAiNgsqRPYQvFOm2URcajplZuZWV2DhntEfJ/a19EB5tVZpx1ob6AuMzNrgD+hamaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWIYe7mVmGHO5mZhlyuJuZZcjhbmaWoUHDXdK1kvZL2lRqmyhpjaTtaTqhdNsVknZI2ibpopEq3MzM6qty5v5lYH6/tsuBrohoBbrSMpJmAouAWWmd5ZLGNa1aMzOrZNBwj4i1wM/7NS8AOtJ8B3BxqX1lRByIiJ3ADmBOk2o1M7OKhnvNfXJE7AVI00mp/Sxgd6lfT2o7gqSlkroldff29g6zDDMzq6XZL6iqRlvU6hgRKyKiLSLaWlpamlyGmdkft+GG+z5JUwDSdH9q7wGmlfpNBfYMvzwzMxuO4Yb7amBxml8MrCq1L5J0oqQZQCuwvrESzcxsqMYP1kHSdcALgTMl9QD/AFwFdEpaAuwCFgJExGZJncAW4CCwLCIOjVDtZmZWx6DhHhGX1rlpXp3+7UB7I0WZmVlj/AlVM7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMOdzNzDLkcDczy5DD3cwsQw53M7MMjVi4S5ovaZukHZIuH6lxzMzsSCMS7pLGAf8KvAyYCVwqaeZIjGVmZkcaqTP3OcCOiHggIn4PrAQWjNBYZmbWjyKi+RuVLgHmR8Rfp+U3As+KiHeU+iwFlqbFc4FtDQ57JvCzBrfRDGOhjrFQA4yNOlzDYWOhjrFQA4yNOppRw5MioqXWDeMb3HA9qtH2mGeRiFgBrGjagFJ3RLQ1a3vHch1joYaxUodrGFt1jIUaxkodI13DSF2W6QGmlZanAntGaCwzM+tnpML9LqBV0gxJJwCLgNUjNJaZmfUzIpdlIuKgpHcANwPjgGsjYvNIjFXStEs8DRoLdYyFGmBs1OEaDhsLdYyFGmBs1DGiNYzIC6pmZnZ0+ROqZmYZcribmWXomAp3SY8c7RqqkvTl9H5/JN0qqeG3PPX9/pKeKOmGND9b0stLfV4o6Tml5Sslva/RsRsh6c2SPn80a6hnLNXW/1gOY/0zJP1tM2sa4vjTJW06WuOPNY3uj0bz7pgK90ZIGvaLxyqMmX0VEXsi4pK0OBsoB8ILgeccsZKNqmHe3/ofy6E6Azhq4X6sS1+bko0xE1gAki6T9K40/xlJt6T5eZK+lubbJd0j6YeSJqe2FknflHRX+pmb2q+UtELSd4GvSBon6erU515JbxuglumStkpaDtwNvFHSHZLulnS9pMelfh9J29uUxqr1Aa6+bS6R9JnS8lslfXoY+2l6Gu8E4KPA6yRtlPQB4O3A36Xl5/Vb7xxJ35G0QdLtkp46xDHvl3RNGvvrkl4iaZ2k7ZLmpJ8fSPpRmp5bYzuvSPvxzHrHrc64HemY3SDpFEnnS7ot/S43S5qS+s9O9417Jd0oaUJqv1XSZ1NdmyTNqTHWoPWU+n441bVG0nWS3pfG+ISk24B3D3C/PGI/1TiWr6t6bEquAs5J61+dfjZJuq9veyoc0d5E4yR9QdJmSd+VdHKtYyJpkqQNqabzJIWks9PyjyWdMpRBNUh2SLpQtR+/D6p4DH8fWNjIY2QI++Ot6f5wT7p/nJJqmZFqvEvSxxocFyJizPwAzwauT/O3A+uB44F/AN5G8SnXV6XbPwX8fZr/BvDcNH82sDXNXwlsAE5Oy0tL65wIdAMz6tQyHXg01XQmsBY4Nd32AeAjaX5iaZ2vlur7MnBJmr8VaANOBX4MHJ/afwA8Ywj755FSbZvS/JuBz5f6XAm8r9Yy0AW0pvlnAbcMYezpwEHgGRQnBRuAayk+jbwA+C/gNGB86v8S4JvlGoHXpOM6YaDjVmPcAOam5WuB96d915LaXkfxdluAe4EXpPmPAp8tHYMvpPnn19p/VepJt7UBG4GTgccD24H3pTGWl/rVu18OuJ8aePyU7xevBdZQvBV5MrALmFKvvUmP3777yOy03Am8YYBjsjnti3dQfDbm9cCTgDuanB0foP7j90HgstJ2hv0YGcL+eEKpz8eBd6b51cCb0vwy0uN9uD8j9fUDw7UBOF/S44EDFGfMbcDzgHcBvwduKvV9aZp/CTBTh0+aT0vbAFgdEb9L8xcCf6p0LRw4HWgFdtap5ycR8UNJr6T4dst1aYwTgDtSnxdJugw4BZhIcYf9dq2NRcRv0hnFKyVtpQj5+wbZJ02RzlSeA1xf2k8nDnEzO/vqlbQZ6IqIkHQfxR35dKBDUitFIB9fWvdFFMfywoj4dWqredwi4uF+4+6OiHVp/mvAB4GnA2vSuuOAvZJOB86IiNtS3w7g+tJ2rgOIiLWSTpN0Rr9xqtbzXGBV3/1KUvl4/+dg22Pg/dQszwWui4hDwL7018SfD9DerA8Z7oyIjWl+A3AO9Y/JD4C5FE+2nwDmU5ws3D6McQfKjtXUf/xCOmZNeoz0139/TAeeLunjFJfRHkfxeSAo9sVr0/xXgU82MvCYCveI+IOkB4G3UBz4eylC4RxgK/CHSE9rwCEO138ccEEpxAFIB+g35SaKZ8mbqaZvXQFrIuLSfts/CVgOtEXEbklXAicNss1rKMLpfuBLFetohuOAX0bE7Aa2caA0/2hp+VGKY/Ex4HsR8RpJ0ynOZPs8ADwZeArFX0x9NR1x3Gro/2GMh4HNEXFBuTGF+1C203+5aj11L73x2Ptbvfvl56i/n5qlXo0D1d4M5fvIIYoAq+d2ivB9ErCK4ow6OHwCV9kg2bGTGo/fkr5j1ozHSH/998fJFH/VXxwR90h6M8XrZH2a9sGjMXXNPVlL8SfuWoqD/3ZgYynUa/kuxZ92QHHdtU6/m4G/kXR86vcUSadWqOmHwFxJf5LWO0XSUzgc5D9Lz/qX1NtAn4i4k+J7d/6KdCbZoIcpLg3UW+4b99fATkkL4f+vvZ7XhPHLTgf+J82/ud9tPwH+guK1j1mprepxO1tSX5BfSnE8WvraJB0vaVZE/Ar4hQ6/1vBG4LbSdvquOz8X+FXqX1a1nu8Dr5J0Ujrur6jTr9726u2nmsduCMrrr6W4fj9OUgvF2fH6AdpHykDHZC3FZYrtEfEo8HOKF5TXHbGVampmB/Ufv48xSo8RKI7R3pRDry+1r6P4qhb6tQ/LWAz32ymuDd4REfuA/2XwP9PeBbSpeMFmC8VBreUaYAtwt4q3KP0HFf56iYheigfhdZLupbizPDUifgl8AbiP4przXYNtK+kE1kXELyr2H8j3KP7073sR7tvAa1TjBVWKO8wSSfdQXD5q9nfsfwr4J0nrKC6VPEZEbEs1XC/pHKoft63A4rTvJwKfo3gi/WT6XTZy+B1Ci4GrU9/ZFNd4+/xC0g+AfweW1BinUj0RcRfFn/r3AN+i+Euk/xPFQNurt5/6H8shiYiHKC49bAIuoDh7vQe4heK68k+BG+u0j6SaxyQiHky3r03T71OcOQ/3cVEzO+o9futsY6QfIwAfBu6keO3j/lL7u4Flku6iOAFoiL9+4CiQdBPwmYjoOtq1jHXpssVNEfH0BrdzK8ULy92D9a24vcdFxCPpnQ5rgaURcXcztm3WDGPxzD1bKj5k8t/A7xzsx7wVkjZSvHD3TQe7jTU+czczy5DP3M3MMuRwNzPLkMPdzCxDDnczsww53M3MMvR/QgTFyXDKyioAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.barplot(x=words, y=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilty functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return the probabilty to have the input word knowing the class\n",
    "#P(wi|Cj)\n",
    "#P('nice'|Positive)\n",
    "# With Laplace Smoothing\n",
    "\n",
    "def proba_word(word,counter):    \n",
    "    \n",
    "    try: \n",
    "        \n",
    "        # If the word is in our Vocabulary\n",
    "        r = (counter[word]+1)/(sum(counter.values())+len(counter))\n",
    "        \n",
    "    except KeyError:\n",
    "        # Else counter[word] = 0 \n",
    "        r = (1)/(sum(counter.values())+len(counter))\n",
    "        \n",
    "    return(np.float64(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the sum of the log probabilities of each word in the input text\n",
    "\n",
    "def proba_text(text,counter):   \n",
    "    \n",
    "    probs=[]\n",
    "    \n",
    "    for word in list(set(text.split())):\n",
    "        probs.append(np.log(proba_word(word,counter)))\n",
    "        \n",
    "    return(np.sum(probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NBmodel(text_array,counter):\n",
    "    \n",
    "    predictions = []\n",
    "    neg_count,pos_count = counter\n",
    "    \n",
    "    for text in text_array:\n",
    "        \n",
    "        probs = [proba_text(text,neg_count),proba_text(text,pos_count)]\n",
    "        predictions.append(np.argmax(probs))\n",
    "        \n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_list = glob.glob(pathname=\"./data/movie-reviews-en/test/pos/*.txt\")\n",
    "neg_test_list = glob.glob(pathname=\"./data/movie-reviews-en/test/neg/*.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_text = get_text_list(pos_test_list)      \n",
    "neg_test_text = get_text_list(neg_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pos_test_text)):\n",
    "    pos_test_text[i] = pos_test_text[i].translate(str.maketrans('','', string.punctuation))\n",
    "    pos_test_text[i] = pos_test_text[i].replace('\\n','')\n",
    "\n",
    "for i in range(0,len(neg_test_text)):\n",
    "    neg_test_text[i] = neg_test_text[i].translate(str.maketrans('','', string.punctuation))\n",
    "    neg_test_text[i] = neg_test_text[i].replace('\\n','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_results = NBmodel(pos_test_text,[neg_count,pos_count])\n",
    "neg_results = NBmodel(neg_test_text,[neg_count,pos_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Negative Accuracy : 0.95 '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\" Negative Accuracy : {(len(neg_results)-np.sum(neg_results))/(len(neg_results))} \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Accuracy : 0.7'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Positive Accuracy : {np.sum(pos_results)/(len(pos_results))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[70, 30],\n",
       "       [ 5, 95]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = np.sum(pos_results)\n",
    "TN = len(neg_results)-np.sum(neg_results)\n",
    "FP = len(pos_results)-np.sum(pos_results)\n",
    "FN = np.sum(neg_results)\n",
    "\n",
    "z = np.array([[TP,FP],[FN,TN]])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global Accuracy : 0.825'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Global Accuracy : {(TP+TN)/(TP+TN+FP+FN)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now with a bit of Preprocessing\n",
    "\n",
    "Removing stop words using ntlk package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "for word in stopwords:\n",
    "    pos_text = re.sub(' '+ word+' ',' ',pos_text)\n",
    "    neg_text = re.sub(' '+ word+' ',' ',neg_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = dict(Counter(pos_text.split()).most_common(n_word))\n",
    "neg_count = dict(Counter(neg_text.split()).most_common(n_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in stopwords:\n",
    "    for i in range(0,len(pos_test_text)):\n",
    "        pos_test_text[i] = re.sub(' '+ word+' ',' ',pos_test_text[i])\n",
    "        neg_test_text[i] = re.sub(' '+ word+' ',' ',neg_test_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative Accuracy : 0.91'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_results = NBmodel(neg_test_text,[neg_count,pos_count])\n",
    "f\"Negative Accuracy : {(len(neg_results)-np.sum(neg_results))/(len(neg_results))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive Accuracy : 0.72'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_results = NBmodel(pos_test_text,[neg_count,pos_count])\n",
    "f\"Positive Accuracy : {np.sum(pos_results)/(len(pos_results))}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[72, 28],\n",
       "       [ 9, 91]], dtype=int64)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = np.sum(pos_results)\n",
    "TN = len(neg_results)-np.sum(neg_results)\n",
    "FP = len(pos_results)-np.sum(pos_results)\n",
    "FN = np.sum(neg_results)\n",
    "\n",
    "z = np.array([[TP,FP],[FN,TN]])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global Accuracy : 0.815'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Global Accuracy : {(TP+TN)/(TP+TN+FP+FN)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Stemmer\n",
    "\n",
    "Testing nltk english stemmer\n",
    "\n",
    "Restart from scratch because we have removed stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "pos_list=glob.glob(pathname=\"./data/movie-reviews-en/train/pos/*.txt\")\n",
    "neg_list=glob.glob(pathname=\"./data/movie-reviews-en/train/neg/*.txt\")\n",
    "\n",
    "pos_text = ' '.join(get_text_list(pos_list))\n",
    "neg_text = ' '.join(get_text_list(neg_list))\n",
    "\n",
    "pos_text = pos_text.translate(str.maketrans('','', string.punctuation))\n",
    "pos_text = pos_text.replace('\\n','')\n",
    "\n",
    "neg_text = neg_text.translate(str.maketrans('','', string.punctuation))\n",
    "neg_text = neg_text.replace('\\n','')\n",
    "\n",
    "stem_pos_text = []\n",
    "stem_neg_text = []\n",
    "EnglishSnowballStemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "for w in pos_text.split():\n",
    "    \n",
    "    if w != ' ':\n",
    "        stemword = EnglishSnowballStemmer.stem(w)\n",
    "        stem_pos_text.append(stemword)\n",
    "        \n",
    "for w in neg_text.split():\n",
    "    \n",
    "    if w != ' ':\n",
    "        stemword = EnglishSnowballStemmer.stem(w)\n",
    "        stem_neg_text.append(stemword)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem_pos_count = dict(Counter(stem_pos_text).most_common(n_word))\n",
    "stem_neg_count = dict(Counter(stem_neg_text).most_common(n_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_list = glob.glob(pathname=\"./data/movie-reviews-en/test/pos/*.txt\")\n",
    "neg_test_list = glob.glob(pathname=\"./data/movie-reviews-en/test/neg/*.txt\")\n",
    "\n",
    "pos_test_text = get_text_list(pos_test_list)      \n",
    "neg_test_text = get_text_list(neg_test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the english snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pos_test_text)):\n",
    "    pos_test_text[i] = pos_test_text[i].translate(str.maketrans('','', string.punctuation))\n",
    "    pos_test_text[i] = pos_test_text[i].replace('\\n','')\n",
    "    pos_test_text[i] = ' '.join(EnglishSnowballStemmer.stem(w) for w in pos_test_text[i].split())\n",
    "\n",
    "for i in range(0,len(neg_test_text)):\n",
    "    neg_test_text[i] = neg_test_text[i].translate(str.maketrans('','', string.punctuation))\n",
    "    neg_test_text[i] = neg_test_text[i].replace('\\n','')\n",
    "    neg_test_text[i] = ' '.join(EnglishSnowballStemmer.stem(w) for w in neg_test_text[i].split())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Accuracy : 0.92\n",
      "Positive Accuracy : 0.73\n"
     ]
    }
   ],
   "source": [
    "pos_results = NBmodel(pos_test_text,[stem_neg_count,stem_pos_count])\n",
    "neg_results = NBmodel(neg_test_text,[stem_neg_count,stem_pos_count])\n",
    "\n",
    "print(f\"Negative Accuracy : {(len(neg_results)-np.sum(neg_results))/(len(neg_results))}\")\n",
    "print(f\"Positive Accuracy : {np.sum(pos_results)/(len(pos_results))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73, 27],\n",
       "       [ 8, 92]], dtype=int64)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = np.sum(pos_results)\n",
    "TN = len(neg_results)-np.sum(neg_results)\n",
    "FP = len(pos_results)-np.sum(pos_results)\n",
    "FN = np.sum(neg_results)\n",
    "\n",
    "z = np.array([[TP,FP],[FN,TN]])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global Accuracy : 0.825'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Global Accuracy : {(TP+TN)/(TP+TN+FP+FN)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "pos_list=glob.glob(pathname=\"./data/movie-reviews-en/train/pos/*.txt\")\n",
    "neg_list=glob.glob(pathname=\"./data/movie-reviews-en/train/neg/*.txt\")\n",
    "\n",
    "pos_text = ' '.join(get_text_list(pos_list))\n",
    "neg_text = ' '.join(get_text_list(neg_list))\n",
    "\n",
    "pos_text = pos_text.translate(str.maketrans('','', string.punctuation))\n",
    "pos_text = pos_text.replace('\\n','')\n",
    "\n",
    "neg_text = neg_text.translate(str.maketrans('','', string.punctuation))\n",
    "neg_text = neg_text.replace('\\n','')\n",
    "\n",
    "lemm_pos_text = []\n",
    "lemm_neg_text = []\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for w in pos_text.split():\n",
    "    \n",
    "    if w != ' ':\n",
    "        lemmword = wordnet_lemmatizer.lemmatize(w)\n",
    "        lemm_pos_text.append(lemmword)\n",
    "        \n",
    "for w in neg_text.split():\n",
    "    \n",
    "    if w != ' ':\n",
    "        lemmword = wordnet_lemmatizer.lemmatize(w)\n",
    "        lemm_neg_text.append(lemmword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_pos_count = dict(Counter(lemm_pos_text).most_common(n_word))\n",
    "lemm_neg_count = dict(Counter(lemm_neg_text).most_common(n_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_test_list = glob.glob(pathname=\"./data/movie-reviews-en/test/pos/*.txt\")\n",
    "neg_test_list = glob.glob(pathname=\"./data/movie-reviews-en/test/neg/*.txt\")\n",
    "\n",
    "pos_test_text = get_text_list(pos_test_list)      \n",
    "neg_test_text = get_text_list(neg_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(pos_test_text)):\n",
    "    pos_test_text[i] = pos_test_text[i].translate(str.maketrans('','', string.punctuation))\n",
    "    pos_test_text[i] = pos_test_text[i].replace('\\n','')\n",
    "    pos_test_text[i] = ' '.join(wordnet_lemmatizer.lemmatize(w) for w in pos_test_text[i].split())\n",
    "\n",
    "for i in range(0,len(neg_test_text)):\n",
    "    neg_test_text[i] = neg_test_text[i].translate(str.maketrans('','', string.punctuation))\n",
    "    neg_test_text[i] = neg_test_text[i].replace('\\n','')\n",
    "    neg_test_text[i] = ' '.join(wordnet_lemmatizer.lemmatize(w) for w in neg_test_text[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Accuracy : 0.94\n",
      "Positive Accuracy : 0.73\n"
     ]
    }
   ],
   "source": [
    "pos_results = NBmodel(pos_test_text,[lemm_neg_count,lemm_pos_count])\n",
    "neg_results = NBmodel(neg_test_text,[lemm_neg_count,lemm_pos_count])\n",
    "\n",
    "print(f\"Negative Accuracy : {(len(neg_results)-np.sum(neg_results))/(len(neg_results))}\")\n",
    "print(f\"Positive Accuracy : {np.sum(pos_results)/(len(pos_results))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[73, 27],\n",
       "       [ 6, 94]], dtype=int64)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = np.sum(pos_results)\n",
    "TN = len(neg_results)-np.sum(neg_results)\n",
    "FP = len(pos_results)-np.sum(pos_results)\n",
    "FN = np.sum(neg_results)\n",
    "\n",
    "z = np.array([[TP,FP],[FN,TN]])\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Global Accuracy : 0.835'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Global Accuracy : {(TP+TN)/(TP+TN+FP+FN)}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
